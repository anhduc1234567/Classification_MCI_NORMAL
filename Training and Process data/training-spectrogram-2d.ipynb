{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.11.11","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"nvidiaTeslaT4","dataSources":[{"sourceId":11839187,"sourceType":"datasetVersion","datasetId":7438362},{"sourceId":11964256,"sourceType":"datasetVersion","datasetId":7523210}],"dockerImageVersionId":31041,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"import os\nfrom PIL import Image\nimport torch\nimport torch.nn as nn\nfrom torchvision import datasets, models, transforms\nfrom torch.utils.data import DataLoader, Subset\nfrom sklearn.model_selection import StratifiedKFold\nimport numpy as np\n","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true,"execution":{"iopub.status.busy":"2025-05-27T07:59:40.922324Z","iopub.execute_input":"2025-05-27T07:59:40.922556Z","iopub.status.idle":"2025-05-27T07:59:53.990690Z","shell.execute_reply.started":"2025-05-27T07:59:40.922519Z","shell.execute_reply":"2025-05-27T07:59:53.990147Z"}},"outputs":[],"execution_count":1},{"cell_type":"code","source":"import os\nfrom PIL import Image\nfrom torchvision import transforms\nfrom torch.utils.data import Dataset\n\nclass EEGDataset(Dataset):\n    def __init__(self, image_paths, labels, transform=None):\n        self.image_paths = image_paths\n        self.labels = labels\n        self.transform = transform\n    \n    def __len__(self):\n        return len(self.image_paths)\n\n    def __getitem__(self, idx):\n        img = Image.open(self.image_paths[idx]).convert(\"RGB\")\n        label = self.labels[idx]\n        if self.transform:\n            img = self.transform(img)\n        return img, label\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-27T08:54:40.147302Z","iopub.execute_input":"2025-05-27T08:54:40.147820Z","iopub.status.idle":"2025-05-27T08:54:40.152740Z","shell.execute_reply.started":"2025-05-27T08:54:40.147797Z","shell.execute_reply":"2025-05-27T08:54:40.151975Z"}},"outputs":[],"execution_count":24},{"cell_type":"code","source":"import re\nfrom collections import defaultdict\n\ndef get_patient_id(filename):\n    match = re.search(r'patient_(\\d+)_', filename)\n    return int(match.group(1)) if match else -1\n\ndef load_data_by_patient_nested(dataset_root):\n    patient_to_images = defaultdict(list)\n    patient_labels = {}\n\n    # Iterate through top-level folders (e.g., NC, MCI)\n    for main_folder in os.listdir(dataset_root):\n        main_path = os.path.join(dataset_root, main_folder)\n        if not os.path.isdir(main_path):\n            continue\n\n        # Assign label based on folder name (1 for MCI, 0 for NC)\n        label = 1 if main_folder == 'MCI' else 0\n\n        # Iterate through files in the folder\n        for fname in os.listdir(main_path):\n            fpath = os.path.join(main_path, fname)\n            if os.path.isfile(fpath):\n                match = re.search(r'patient_(\\d+)_', fname)\n                if match:\n                    pid = int(match.group(1))\n                    patient_to_images[pid].append(fpath)\n                    patient_labels[pid] = label\n\n    return patient_to_images, patient_labels\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-27T08:54:41.955648Z","iopub.execute_input":"2025-05-27T08:54:41.955879Z","iopub.status.idle":"2025-05-27T08:54:41.961725Z","shell.execute_reply.started":"2025-05-27T08:54:41.955864Z","shell.execute_reply":"2025-05-27T08:54:41.961097Z"}},"outputs":[],"execution_count":25},{"cell_type":"code","source":"from sklearn.model_selection import StratifiedKFold\nfrom torch.utils.data import DataLoader, Subset\nimport numpy as np\nfrom sklearn.model_selection import train_test_split\n\ndef create_folds(patient_to_images, patient_labels, transform, k=5):\n    patient_ids = list(patient_to_images.keys())\n    patient_y = [patient_labels[pid] for pid in patient_ids]\n\n    skf = StratifiedKFold(n_splits=k, shuffle=True, random_state=42)\n    folds = []\n\n    for train_idx, val_idx in skf.split(patient_ids, patient_y):\n        train_img_paths, train_labels = [], []\n        val_img_paths, val_labels = [], []\n\n        for i in train_idx:\n            pid = patient_ids[i]\n            train_img_paths += patient_to_images[pid]\n            train_labels += [patient_labels[pid]] * len(patient_to_images[pid])\n\n        for i in val_idx:\n            pid = patient_ids[i]\n            val_img_paths += patient_to_images[pid]\n            val_labels += [patient_labels[pid]] * len(patient_to_images[pid])\n\n        train_dataset = EEGDataset(train_img_paths, train_labels, transform)\n        val_dataset = EEGDataset(val_img_paths, val_labels, transform)\n\n        folds.append((train_dataset, val_dataset))\n\n    return folds\n\ndef create_train_test_split_all_bands(patient_to_images, patient_labels, transform, test_size=0.2):\n    patient_ids = list(patient_to_images.keys())\n    patient_y = [patient_labels[pid] for pid in patient_ids]\n\n    # Keep all images without filtering for specific bands\n    all_images = patient_to_images\n\n    # Filter out patients with no images\n    filtered_pids = [pid for pid in patient_ids if len(all_images[pid]) > 0]\n    filtered_y = [patient_labels[pid] for pid in filtered_pids]\n\n    # Split patient_ids into train and test sets\n    train_idx, test_idx = train_test_split(\n        range(len(filtered_pids)),\n        test_size=test_size,\n        stratify=filtered_y,\n        random_state=42\n    )\n\n    # Create lists of image paths and labels for train set\n    train_img_paths, train_labels = [], []\n    for i in train_idx:\n        pid = filtered_pids[i]\n        train_img_paths += all_images[pid]\n        train_labels += [patient_labels[pid]] * len(all_images[pid])\n\n    # Create lists of image paths and labels for test set\n    test_img_paths, test_labels = [], []\n    for i in test_idx:\n        pid = filtered_pids[i]\n        test_img_paths += all_images[pid]\n        test_labels += [patient_labels[pid]] * len(all_images[pid])\n\n    # Create datasets for train and test\n    train_dataset = EEGDataset(train_img_paths, train_labels, transform)\n    test_dataset = EEGDataset(test_img_paths, test_labels, transform)\n\n    return train_dataset, test_dataset","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-27T08:54:43.423499Z","iopub.execute_input":"2025-05-27T08:54:43.423986Z","iopub.status.idle":"2025-05-27T08:54:43.432892Z","shell.execute_reply.started":"2025-05-27T08:54:43.423957Z","shell.execute_reply":"2025-05-27T08:54:43.432192Z"}},"outputs":[],"execution_count":26},{"cell_type":"code","source":"from torchvision import models\nimport torch.nn as nn\nimport torch\nimport torch\nimport torch.nn as nn\nimport torch.nn.functional as F\nfrom torchvision import models, transforms\nfrom torch.utils.data import DataLoader\nimport numpy as np\nimport cv2\nimport matplotlib.pyplot as plt\nfrom PIL import Image\n\ndef get_pretrained_model():\n    model = models.resnet18(pretrained=True)\n    model.fc = nn.Sequential(\n        nn.Dropout(0.3),\n        nn.Linear(model.fc.in_features, 2)\n    )\n    return model\n\nclass CAMModel(nn.Module):\n    def __init__(self, model):\n        super(CAMModel, self).__init__()\n        self.resnet = model\n        self.features = nn.Sequential(*list(model.children())[:-2])  # Extract features up to last conv layer\n        self.avgpool = model.avgpool\n        self.fc = model.fc\n        self.last_conv_layer = list(model.children())[-3][-1]  # Last block of ResNet18 layer4\n\n    def forward(self, x):\n        x = self.features(x)\n        feature_maps = x\n        x = self.avgpool(x)\n        x = x.view(x.size(0), -1)\n        x = self.fc(x)\n        return x, feature_maps\n\n    def get_cam(self, feature_maps, weights, class_idx, input_size=(224, 224)):\n        batch_size, channels, h, w = feature_maps.size()\n        cam = weights[class_idx].unsqueeze(0) @ feature_maps.view(batch_size, channels, -1)\n        cam = cam.view(batch_size, h, w)\n        cam = cam - cam.min()\n        cam = cam / cam.max()\n        cam = F.interpolate(cam.unsqueeze(0), size=input_size, mode='bilinear', align_corners=False)\n        return cam.squeeze().cpu().numpy()\n\ndef visualize_cam(image, cam, save_path, alpha=0.4):\n    image = image.permute(1, 2, 0).cpu().numpy()\n    image = (image * 0.5 + 0.5) * 255  # Denormalize\n    image = image.astype(np.uint8)\n    \n    cam = cv2.resize(cam, (image.shape[1], image.shape[0]))\n    heatmap = cv2.applyColorMap(np.uint8(255 * cam), cv2.COLORMAP_JET)\n    heatmap = np.float32(heatmap) / 255\n    superimposed_img = heatmap * alpha + image / 255\n    superimposed_img = superimposed_img / np.max(superimposed_img)\n    \n    plt.figure(figsize=(8, 8))\n    plt.imshow(superimposed_img)\n    plt.axis('off')\n    plt.savefig(save_path, bbox_inches='tight')\n    plt.close()\n\ndef train_model(model, train_loader, val_loader, device, cam_output_dir=\"cam_outputs\"):\n    import os\n    os.makedirs(cam_output_dir, exist_ok=True)\n    \n    cam_model = CAMModel(model).to(device)\n    optimizer = torch.optim.Adam(cam_model.parameters(), lr=1e-4, weight_decay=1e-5)\n    criterion = nn.CrossEntropyLoss()\n    best_val_acc = 0\n\n    # Get weights of the final FC layer for CAM\n    fc_weights = cam_model.fc[1].weight.data  # Shape: [2, 512]\n\n    for epoch in range(30):\n        cam_model.train()\n        total, correct = 0, 0\n        for X, y in train_loader:\n            X, y = X.to(device), y.to(device)\n            optimizer.zero_grad()\n            out, _ = cam_model(X)\n            loss = criterion(out, y)\n            loss.backward()\n            optimizer.step()\n            correct += (out.argmax(1) == y).sum().item()\n            total += y.size(0)\n\n        val_acc = evaluate(cam_model, val_loader, device, cam_output_dir, fc_weights,epoch)\n        print(f\"Epoch {epoch+1} | Train Acc: {correct/total:.4f} | Val Acc: {val_acc:.4f}\")\n\n        if val_acc > best_val_acc:\n            best_val_acc = val_acc\n            torch.save(cam_model.resnet.state_dict(), \"best_model_fold.pth\")\n    return cam_model\ndef evaluate(model, loader, device, cam_output_dir, fc_weights, epoch):\n    model.eval()\n    correct, total = 0, 0\n    with torch.no_grad():\n        for batch_idx, (X, y) in enumerate(loader):\n            X, y = X.to(device), y.to(device)\n            out, feature_maps = model(X)\n            pred = out.argmax(1)\n            correct += (pred == y).sum().item()\n            total += y.size(0)\n\n            # Generate CAM for first image in batch\n            if batch_idx == 0:  # Only for first batch to avoid too many images\n                for i in range(min(3, X.size(0))):  # Visualize up to 3 images\n                    cam = model.get_cam(feature_maps[i:i+1], fc_weights, pred[i].item())\n                    visualize_cam(X[i], cam, f\"{cam_output_dir}/cam_epoch_{epoch+1}_img_{i}.png\")\n\n    return correct / total","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-27T08:54:50.659457Z","iopub.execute_input":"2025-05-27T08:54:50.659725Z","iopub.status.idle":"2025-05-27T08:54:50.675853Z","shell.execute_reply.started":"2025-05-27T08:54:50.659706Z","shell.execute_reply":"2025-05-27T08:54:50.674990Z"}},"outputs":[],"execution_count":28},{"cell_type":"code","source":"dataset_root = '/kaggle/input/newdata/newdata'\npatient_to_images, patient_labels = load_data_by_patient_nested(dataset_root)\n\ntransform = transforms.Compose([\n    transforms.Resize((224, 224)),\n    transforms.ToTensor(),\n    transforms.Normalize([0.5], [0.5])\n])\n\ntrain_dataset, val_dataset = create_train_test_split_all_bands(patient_to_images, patient_labels, transform)\n\ndevice = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n\ntrain_loader = DataLoader(train_dataset, batch_size=32, shuffle=True)\nval_loader = DataLoader(val_dataset, batch_size=32, shuffle=False)\n\nmodel = get_pretrained_model()\nmodel = train_model(model, train_loader, val_loader, device)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-27T08:54:55.575792Z","iopub.execute_input":"2025-05-27T08:54:55.576060Z","iopub.status.idle":"2025-05-27T09:21:40.505570Z","shell.execute_reply.started":"2025-05-27T08:54:55.576039Z","shell.execute_reply":"2025-05-27T09:21:40.504967Z"}},"outputs":[{"name":"stderr","text":"/usr/local/lib/python3.11/dist-packages/torchvision/models/_utils.py:208: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and may be removed in the future, please use 'weights' instead.\n  warnings.warn(\n/usr/local/lib/python3.11/dist-packages/torchvision/models/_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=ResNet18_Weights.IMAGENET1K_V1`. You can also use `weights=ResNet18_Weights.DEFAULT` to get the most up-to-date weights.\n  warnings.warn(msg)\n","output_type":"stream"},{"name":"stdout","text":"Epoch 1 | Train Acc: 0.5679 | Val Acc: 0.4859\nEpoch 2 | Train Acc: 0.6229 | Val Acc: 0.5282\nEpoch 3 | Train Acc: 0.6632 | Val Acc: 0.5487\nEpoch 4 | Train Acc: 0.7092 | Val Acc: 0.5032\nEpoch 5 | Train Acc: 0.7668 | Val Acc: 0.4955\nEpoch 6 | Train Acc: 0.8330 | Val Acc: 0.5109\nEpoch 7 | Train Acc: 0.8769 | Val Acc: 0.5821\nEpoch 8 | Train Acc: 0.9054 | Val Acc: 0.4968\nEpoch 9 | Train Acc: 0.9191 | Val Acc: 0.5109\nEpoch 10 | Train Acc: 0.9432 | Val Acc: 0.5179\nEpoch 11 | Train Acc: 0.9516 | Val Acc: 0.5077\nEpoch 12 | Train Acc: 0.9599 | Val Acc: 0.5538\nEpoch 13 | Train Acc: 0.9585 | Val Acc: 0.5212\nEpoch 14 | Train Acc: 0.9641 | Val Acc: 0.5276\nEpoch 15 | Train Acc: 0.9792 | Val Acc: 0.5096\nEpoch 16 | Train Acc: 0.9781 | Val Acc: 0.5327\nEpoch 17 | Train Acc: 0.9738 | Val Acc: 0.4994\nEpoch 18 | Train Acc: 0.9703 | Val Acc: 0.5378\nEpoch 19 | Train Acc: 0.9809 | Val Acc: 0.5064\nEpoch 20 | Train Acc: 0.9811 | Val Acc: 0.5096\nEpoch 21 | Train Acc: 0.9842 | Val Acc: 0.4962\nEpoch 22 | Train Acc: 0.9780 | Val Acc: 0.5192\nEpoch 23 | Train Acc: 0.9809 | Val Acc: 0.5442\nEpoch 24 | Train Acc: 0.9804 | Val Acc: 0.5192\nEpoch 25 | Train Acc: 0.9800 | Val Acc: 0.5256\nEpoch 26 | Train Acc: 0.9828 | Val Acc: 0.5224\nEpoch 27 | Train Acc: 0.9882 | Val Acc: 0.5340\nEpoch 28 | Train Acc: 0.9936 | Val Acc: 0.5628\nEpoch 29 | Train Acc: 0.9911 | Val Acc: 0.5288\nEpoch 30 | Train Acc: 0.9833 | Val Acc: 0.5564\n","output_type":"stream"}],"execution_count":29},{"cell_type":"code","source":"import torch.nn.functional as F\n\ndef get_predictions(model, test_dataset, batch_size=32):\n    test_loader = DataLoader(test_dataset, batch_size=batch_size, shuffle=False)\n    model.eval()\n    all_preds = []\n    all_labels = []\n    \n    with torch.no_grad():\n        for images, labels in test_loader:\n            images = images.to(device)\n            labels = labels.to(device)\n            outputs,f = model(images)  # Đầu ra là logits\n            pred = outputs.argmax(1)  # Lấy xác suất lớp dương (1) cho bài toán nhị phân\n            all_preds.extend(pred.cpu().numpy())\n            all_labels.extend(labels.cpu().numpy())\n    \n    return np.array(all_labels), np.array(all_preds)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-27T09:30:33.474442Z","iopub.execute_input":"2025-05-27T09:30:33.475080Z","iopub.status.idle":"2025-05-27T09:30:33.479965Z","shell.execute_reply.started":"2025-05-27T09:30:33.475057Z","shell.execute_reply":"2025-05-27T09:30:33.479232Z"}},"outputs":[],"execution_count":38},{"cell_type":"code","source":"from sklearn.metrics import roc_curve, auc, confusion_matrix\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nimport numpy as np\n\n# Lấy dự đoán xác suất và nhãn thực\ny_true, y_pred_proba = get_predictions(model, val_dataset)\ny_pred = (y_pred_proba > 0.5).astype(int)\n\n# Tính confusion matrix\ncm = confusion_matrix(y_true, y_pred)\ntn, fp, fn, tp = cm.ravel()\n\n# Tính các chỉ số đánh giá\nsensitivity = tp / (tp + fn) if (tp + fn) > 0 else 0  # Recall\nspecificity = tn / (tn + fp) if (tn + fp) > 0 else 0\nprecision = tp / (tp + fp) if (tp + fp) > 0 else 0\nf1_score = 2 * (precision * sensitivity) / (precision + sensitivity) if (precision + sensitivity) > 0 else 0\n\n# ROC và AUC\nfpr, tpr, _ = roc_curve(y_true, y_pred_proba)\nroc_auc = auc(fpr, tpr)\n\n# Vẽ ROC curve\nplt.figure()\nplt.plot(fpr, tpr, color='darkorange', lw=2, label=f'ROC curve (AUC = {roc_auc:.2f})')\nplt.plot([0, 1], [0, 1], color='navy', lw=2, linestyle='--')\nplt.xlim([0.0, 1.0])\nplt.ylim([0.0, 1.05])\nplt.xlabel('False Positive Rate')\nplt.ylabel('True Positive Rate')\nplt.title('Receiver Operating Characteristic (ROC) Curve')\nplt.legend(loc=\"lower right\")\nplt.grid(True)\nplt.savefig('roc2d.png', bbox_inches='tight')\nplt.show()\n\n# Vẽ Confusion Matrix\nplt.figure()\nsns.heatmap(cm, annot=True, fmt='d', cmap='Blues', \n            xticklabels=['Predicted 0', 'Predicted 1'],\n            yticklabels=['Actual 0', 'Actual 1'])\nplt.xlabel('Predicted Label')\nplt.ylabel('True Label')\nplt.title('Confusion Matrix')\nplt.savefig('confusion_matrix.png', bbox_inches='tight')\nplt.show()\n\n# In kết quả\nprint(f\"Sensitivity (Recall): {sensitivity:.2f}\")\nprint(f\"Specificity: {specificity:.2f}\")\nprint(f\"Precision: {precision:.2f}\")\nprint(f\"F1 Score: {f1_score:.2f}\")\nprint(f\"AUC: {roc_auc:.2f}\")\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"def visualize_cam2(image, cam,i, alpha=0.4):\n    image = image.permute(1, 2, 0).cpu().numpy()\n    image = (image * 0.5 + 0.5) * 255  # Denormalize\n    image = image.astype(np.uint8)\n    \n    cam = cv2.resize(cam, (image.shape[1], image.shape[0]))\n    heatmap = cv2.applyColorMap(np.uint8(255 * cam), cv2.COLORMAP_JET)\n    heatmap = np.float32(heatmap) / 255\n    superimposed_img = heatmap * alpha + image / 255\n    superimposed_img = superimposed_img / np.max(superimposed_img)\n    \n    plt.figure(figsize=(6, 6))\n    plt.subplot(1,2,1)\n    plt.imshow(superimposed_img), plt.axis('off'),plt.title(\"CAM-image\")\n    plt.subplot(1,2,2)\n    plt.imshow(image), plt.axis('off'),plt.title('Original image')\n    # plt.show()\n    plt.savefig(f'cam2{i}.png', bbox_inches='tight')\n    plt.close()\n    \ndef draw_cam_evaluate(model, loader, device='cuda'):\n    fc_weights = model.fc[1].weight.data  \n    model.eval()\n    correct, total = 0, 0\n    with torch.no_grad():\n        for batch_idx, (X, y) in enumerate(loader):\n            X, y = X.to(device), y.to(device)\n            out, feature_maps = model(X)\n            pred = out.argmax(1)\n            correct += (pred == y).sum().item()\n            total += y.size(0)\n            if batch_idx == 0:  # Only for first batch to avoid too many images\n                for i in range(min(4, X.size(0))):  # Visualize up to 3 images\n                    cam = model.get_cam(feature_maps[i:i+1], fc_weights, pred[i].item())\n                    visualize_cam2(X[i], cam,i)\n\n    return correct / total\ndraw_cam_evaluate(model,val_loader)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-27T09:35:25.935393Z","iopub.execute_input":"2025-05-27T09:35:25.935998Z","iopub.status.idle":"2025-05-27T09:35:35.762523Z","shell.execute_reply.started":"2025-05-27T09:35:25.935975Z","shell.execute_reply":"2025-05-27T09:35:35.761956Z"}},"outputs":[{"execution_count":49,"output_type":"execute_result","data":{"text/plain":"0.5564102564102564"},"metadata":{}}],"execution_count":49}]}